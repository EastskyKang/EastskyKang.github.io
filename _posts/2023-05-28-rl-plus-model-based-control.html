---
title: "RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion"
<!-- description: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022) -->
layout: distill
published: true
date: 2023-05-27 16:10:33
img: /assets/img/2023-05-28/kang2023rl.png
permalink: /rl-plus-model-based-control/
tags: [robotics, publication]
authors:
  - name: Dongho Kang
    url: "https://donghok.me/"
    affiliations:
      name: CRL<d-footnote>Computational Robotics Lab.</d-footnote>, ETH Zurich
  - name: Jin Cheng
    affiliations:
      name: CRL, ETH Zurich
  - name: Miguel Zamora
    affiliations:
      name: CRL, ETH Zurich
  - name: Fatemeh Zargarbashi
    affiliations:
      name: CRL, ETH Zurich
  - name: Stelian Coros
    url: "http://crl.ethz.ch/"
    affiliations:
      name: CRL, ETH Zurich
bibliography:
---

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      <img class="img-fluid" src="{{ '/assets/img/2023-05-28/kang2023rl.png' | relative_url }}" alt="" title="teaser"/>
  </div>
</div>

<p> <!-- empty space for margin --> </p> 

<h1>Abstract</h1>

<p>
This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. 
Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits.
These reference motions serve as targets for the RL policy to imitate, resulting in the development of robust control policies that can be learned efficiently and reliably. 
Moreover, by considering whole-body dynamics, RL overcomes the inherent limitations of modelling simplifications.
Through simulation and hardware experiments, we demonstrate the robustness and controllability of the RL training process within our framework. 
Furthermore, our method demonstrates the ability to generalize reference motions and handle more complex locomotion tasks that may pose challenges for the simplified model, leveraging the flexibility of RL.
</p>

<p>
  <b>Preprint: [<a href="{{ "/assets/pdf/kang2023rl.pdf" | relative_url }}">PDF</a>] / [<a href="https://arxiv.org/abs/2305.17842">ArXiv</a>]</b> 
</p>

<hr>

<h1>Supplementary Video</h1>

<p>
{% include youtube_player.html id="gXDP87yVq4o" %}
</p>

<!-- <hr>

<h1>Presentation</h1>

<p>
</p> -->

<hr>

<h1>Bibtex</h1>

<p>
{% highlight txt %}
@misc{kang2023rl,
  title={RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion}, 
  author={Kang, Dongho and Cheng, Jin and Zamora, Miguel and Zargarbashi, Fatemeh and Coros, Stelian},
  year={2023},
  eprint={2305.17842},
  archivePrefix={arXiv},
  primaryClass={cs.RO}
}
{% endhighlight %}
</p>

<hr>

<h1>Demos</h1>

<p>
  {% include youtube_player.html id="PCT5f6xsASk" %}
</p>  

<hr>
<h1>Acknowledgment</h1>

<p>
This work has received funding from the European Research Council (ERC) under the European Unionâ€™s Horizon 2020 research and innovation programme (grant agreement No. 866480.)
</p>

<p>
We express our gratitude to Zijun Hui for his assistance with the robot experiments.
</p>
  